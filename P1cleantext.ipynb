{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "Poggioni.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfxMB2n9bX2x"
      },
      "source": [
        "Leggere il txt cercando di non salvare le righe che comprendono solo invii."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VbGYVUiIAvj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6752aa2b-619d-4ace-a449-9a590ecbbb74"
      },
      "source": [
        "var_openTXT = open('/content/drive/My Drive/poggioni/training.txt','r').readlines()\n",
        "var_cleanTXT = []\n",
        "for i in range(0,len(var_openTXT)):\n",
        "  if ((var_openTXT[i]) != '\\n'):\n",
        "    var_cleanTXT.append(var_openTXT[i])\n",
        "#for i in range(0,len(var_cleanTXT)):\n",
        "print(var_cleanTXT[2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Anche tu di ritorno dal regno dell'Ade?\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0vsGOGpbqiH"
      },
      "source": [
        "Creazione del csv sfruttando la struttura di questo pseudo xml  ottenendo vari dai inseriti nel file txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Kcqli5OOn5f"
      },
      "source": [
        "import csv\n",
        "post = \"\"\n",
        "topic= \"\"\n",
        "gander = \"\"\n",
        "top_lenpost = 0\n",
        "flag = True \n",
        "lista_topic = ['ANIME','MEDICINE-AESTHETICS','AUTO-MOTO','SPORTS','SMOKE','METAL-DETECTING','CELEBRITIES','ENTERTAINMENT','TECHNOLOGY','NATURE','BIKES']\n",
        "lista_age = ['0-19','20-29','30-39','40-49','50-100']\n",
        "with open('training.csv','w', newline='') as f:\n",
        "  fieldnames = ['post', 'topic','age','gender', 'number word']\n",
        "  thewriter = csv.DictWriter(f,fieldnames=fieldnames)\n",
        "  thewriter.writeheader()\n",
        "  for i in range (0,len(var_cleanTXT)):\n",
        "  #for i in range (0,10000):\n",
        "    if '<user' in var_cleanTXT[i]:\n",
        "      #salva i dati utili e poi continua\n",
        "      dati_utili = var_cleanTXT[i].split(\" \")\n",
        "\n",
        "      #PARTE RIGUARDANTE IL TOPIC\n",
        "      var = dati_utili[2].split('\"')\n",
        "      topic = var[1]\n",
        "      print(dati_utili)\n",
        "      if topic not in lista_topic:\n",
        "        print(\"ERRORE NEL TOPIC\")\n",
        "      #---------------------------\n",
        "      #PARTE RIGUARDANTE IL AGE\n",
        "      var = dati_utili[3].split('\"')\n",
        "      age = var[1]\n",
        "      if age not in lista_age:\n",
        "        print(\"ERRORE NEL AGE\")\n",
        "      #---------------------------\n",
        "      #PARTE RIGUARDANTE IL GANDER\n",
        "      var =dati_utili[4].split('\"')\n",
        "      gender = var[1]\n",
        "      if gender != \"M\" and gender != \"F\":\n",
        "        print(\"ERRORE NEL GANDER\")\n",
        "      if gender == \"M\":\n",
        "        gender = 0\n",
        "      if gender == \"F\":\n",
        "        gender = 1\n",
        "      #----------------------------\n",
        "      print(\"valore di i -->\",i)\n",
        "      flag = False\n",
        "    if '<post>\\n' in var_cleanTXT[i]:\n",
        "      #controlla che la variabile post sia pulita in caso continua\n",
        "      if (post != \"\" ):\n",
        "        print(\"ERRORE\")\n",
        "        print(\"valore di i -->\",i)\n",
        "      flag = True\n",
        "      continue\n",
        "    if '</post>\\n' in var_cleanTXT[i]:\n",
        "      #scrivi sul csv e pulisci la variabile\n",
        "      if (post[-1:]=='\\n'):\n",
        "        #print(\"ho trovato un invio finale\")\n",
        "        post = post[:-1]\n",
        "      thewriter.writerow({'post': post, 'topic': lista_topic.index(topic) ,'age' : lista_age.index(age) ,'gender' : gender,'number word':int(len(post))})\n",
        "      #thewriter.writerow({'post': post, 'topic': topic ,'age' : age ,'gender' : gender,'number word':int(len(post))})\n",
        "      if top_lenpost < len(post):\n",
        "        top_lenpost = len(post)\n",
        "      #print(\"ho scritto sul csv --> \" , post)\n",
        "      post = \"\"    \n",
        "      flag = False\n",
        "    #siamo nel caso dei primi tre che sono risultati falsi\n",
        "    #scrivi sulla variabile post il contenuto della riga \n",
        "    if flag: \n",
        "      post =  post + var_cleanTXT[i]\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjfC9ELkfQHu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc7d67e9-4200-4058-a4d6-a90d95799a34"
      },
      "source": [
        "print(\"il post più lungo è\")\n",
        "print(top_lenpost)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "il post più lungo è\n",
            "50135\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-6FEd39b4QA"
      },
      "source": [
        "Visualizzazione delle prime dieci righe del nuovo csv per rendersi conto della struttura di quest'ultimo. (google colab non apre il csv in visualizzazione perchè troppo grande)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzxt3nqtUsdp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ebde71f-11e1-4342-e4bd-3d55bd3f19eb"
      },
      "source": [
        "import pandas as pd\n",
        "traning = pd.read_csv('/content/training.csv')\n",
        "df = pd.DataFrame(traning)\n",
        "print(df.head(10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                      post  topic  age  gender  number word\n",
            "0  Anche tu di ritorno dal regno dell'Ade?      0    1       0           39\n",
            "1                         Benvenuto, Uomo!      0    1       0           16\n",
            "2                         Benvenuto, Uomo!      0    1       0           16\n",
            "3                         Benvenuto, Uomo!      0    1       0           16\n",
            "4                         Benvenuto, Uomo!      0    1       0           16\n",
            "5                        Bentornato, Uomo!      0    1       0           17\n",
            "6                        Benvenuta...Donna      0    1       0           17\n",
            "7                                AHAHAH!!!      0    1       0            9\n",
            "8                                Ciao Uomo      0    1       0            9\n",
            "9                                Ciao Uomo      0    1       0            9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quiCXi12lebi"
      },
      "source": [
        "with open('/content/training.csv','r') as in_file, open('traning_noduplicate.csv','w') as out_file:\n",
        "    seen = set() # set for fast O(1) amortized lookup\n",
        "    for line in in_file:\n",
        "        if line in seen: continue # skip duplicate\n",
        "\n",
        "        seen.add(line)\n",
        "        out_file.write(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPBGWGAhni15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4d62c17-54ec-47c5-f5ab-83c46142b74a"
      },
      "source": [
        "traning = pd.read_csv('/content/traning_noduplicate.csv')\n",
        "df = pd.DataFrame(traning)\n",
        "print(df.head(20))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                 post  ...  number word\n",
            "0             Anche tu di ritorno dal regno dell'Ade?  ...           39\n",
            "1                                    Benvenuto, Uomo!  ...           16\n",
            "2                                   Bentornato, Uomo!  ...           17\n",
            "3                                   Benvenuta...Donna  ...           17\n",
            "4                                           AHAHAH!!!  ...            9\n",
            "5                                           Ciao Uomo  ...            9\n",
            "6                                          Ciao donna  ...           10\n",
            "7                                       Aug!!! uri...  ...           13\n",
            "8                            Benvenuto, piccolo uomo.  ...           24\n",
            "9                                    Ciao Auguroni...  ...           16\n",
            "10  Maluska 22 Princ 24 WandeFullStar 25 Kuja 27 A...  ...          428\n",
            "11  OK...spero che nessun altro abbia scelto Pavon...  ...          178\n",
            "12  Grazie per l'intervento kim... davvero notevol...  ...          305\n",
            "13                         Staremo a vedere allora...  ...           26\n",
            "14          Ottimo... ...i Saint sono per forza fighi  ...           41\n",
            "15  Spiacente ma non intendo competere per l'inves...  ...          211\n",
            "16  Ciao a tutti... Ho deciso, se per voi va bene,...  ...          107\n",
            "17                                             Grazie  ...            6\n",
            "18  Ciao io sono Argonitz, iscritto da poco....son...  ...           90\n",
            "19                    Ho appena postato la mia scheda  ...           31\n",
            "\n",
            "[20 rows x 5 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBPYT5LaZoEv"
      },
      "source": [
        "num_topic=[0,0,0,0,0,0,0,0,0,0,0]\n",
        "num_word_topic=[0,0,0,0,0,0,0,0,0,0,0]\n",
        "num_age=[0,0,0,0,0]\n",
        "num_word_age=[0,0,0,0,0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yV0fH4vLcgMO"
      },
      "source": [
        "Pre processing del testo con relativa pulizia per ottenere un risultato migliore in termini di acuratezza "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IUcoer4mUhp"
      },
      "source": [
        "#continuare con la pulizia del testo e il conteggio dei smile \n",
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.stem import PorterStemmer #https://www.nltk.org/howto/stem.html\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.snowball import ItalianStemmer\n",
        "import re\n",
        "\n",
        "#per torliere la punteggiatura\n",
        "import string\n",
        "punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_...~''' \n",
        "nltk.download('punkt')\n",
        "#download the stopwords\n",
        "nltk.download('stopwords')\n",
        "#load the italian stopwords\n",
        "stops = set(stopwords.words(\"italian\"))\n",
        "stopsuk = set(stopwords.words(\"english\"))\n",
        "#rimuovo dalle stop word alcune parole per non compromettere la predizione del sesso \n",
        "stops.remove('uno')\n",
        "stops.remove('una')\n",
        "stops.remove('quella')\n",
        "stops.remove('quello')\n",
        "#cerco le emoji\n",
        "!pip install demoji\n",
        "import demoji \n",
        "demoji.download_codes()\n",
        "numero_vuoti=[0]\n",
        "numero_frasi=[0]\n",
        "#-----------------------------------------------------------------------------------------------#\n",
        "def apply_cleaning_function_to_list(X):\n",
        "    cleaned_X = []\n",
        "    for element in X:\n",
        "        cleaned_X.append(clean_text(element))\n",
        "    return cleaned_X\n",
        "\n",
        "def clean_text(raw_text):\n",
        "  \n",
        "  string = raw_text\n",
        "  \n",
        "  #cancello le emoji\n",
        "  #demoji.replace(string,\"\")\n",
        "  #rimpiazzare le emoji con la loro descrizione\n",
        "  demoji.replace_with_desc(string,\"\")\n",
        "  url = Find(string)\n",
        "  for i in range(0,len(url)):\n",
        "    string = string.replace(url[i],'')\n",
        "  #eseguo il tokeniker tramite le parole italiane\n",
        "  tokens = nltk.word_tokenize(string, language='italian')\n",
        "  \n",
        "  #elimino le stopwords italiane e le parole più corte di 2\n",
        "  text_nostopwordsit = [ w for w in tokens if not w.lower() in stops and len(w)>2]\n",
        "  \n",
        "  #elimino le stopword inglesi (ho notaato che alcuni post sono scritti in parte in inglese)\n",
        "  text_nostopwords = [ w for w in text_nostopwordsit if not w.lower() in stopsuk ]\n",
        "\n",
        "  # rendo tutte le lettere minuscole\n",
        "  for i in range(0, len(text_nostopwords)):\n",
        "    text_nostopwords[i]=text_nostopwords[i].lower()\n",
        "  \n",
        "  #vado a eliminare i tag ad altri account \n",
        "  for i in range(0, len(text_nostopwords)):\n",
        "    string = text_nostopwords[i]\n",
        "    if string[0] == '@':\n",
        "      text_nostopwords[i] = ''\n",
        "\n",
        "  #vado ad eliminare la punteggiatura\n",
        "  text_nopunct = [ w for w in text_nostopwords if not w in punctuations]\n",
        "  \n",
        "  #vado ad eliminare i numeri presenti nel testo\n",
        "  text_nonumber = [w for w in text_nopunct if  w.isalpha() ] \n",
        "\n",
        "  #\"riatacco\" le parole precedentemente separate per effettuare la pulizia \n",
        "  joined_words = ( \" \".join(text_nonumber))\n",
        "\n",
        "  if joined_words == \"\":\n",
        "    numero_vuoti.append(0)\n",
        "    #print(joined_words)\n",
        "  numero_frasi.append(0)\n",
        "  #ritorno la stringa da scrivere nel csv \n",
        "  return joined_words\n",
        "\n",
        "def Find(string): \n",
        "\n",
        "\t# findall() has been used \n",
        "\t# with valid conditions for urls in string \n",
        "\tregex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
        "\turl = re.findall(regex,string)\n",
        "\treturn [x[0] for x in url] \n",
        "\n",
        "\n",
        "\n",
        "#------------------------------------------------------------------------------------------------#\n",
        "\n",
        "imdb = pd.read_csv('/content/traning_noduplicate.csv')\n",
        "\n",
        "\n",
        "#imdb = imdb.head(15) #serve per prendere solo le righe indicate \n",
        "\n",
        "# Get text to clean\n",
        "text_to_clean = list(imdb['post'])\n",
        "\n",
        "\n",
        "for i in range (0, len(text_to_clean)):\n",
        "  text_to_clean[i] = str(text_to_clean[i])\n",
        "# Clean text\n",
        "cleaned_text = apply_cleaning_function_to_list(text_to_clean)\n",
        "\n",
        "# Show example\n",
        "print ('Original text:',text_to_clean[15])\n",
        "print ('\\nCleaned text:', cleaned_text[15])\n",
        "\n",
        "\n",
        "\n",
        "# Add cleaned data back into DataFrame\n",
        "imdb['post'] = cleaned_text\n",
        "imdb.to_csv('traning_clean.csv', index=False) #True per avere id prima della frase\n",
        "print(len(numero_vuoti))\n",
        "print(len(numero_frasi))\n",
        "#togliere i duplicati"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZ8FTSGUM83f"
      },
      "source": [
        "for i in range(0,200):\n",
        "  print(cleaned_text[i],i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qy34HHI6AE-o"
      },
      "source": [
        "import csv\n",
        "\n",
        "with open(\"/content/traning_clean.csv\", 'r') as f:\n",
        "  data = list(csv.reader(f))\n",
        "print(data[141])\n",
        "with open(\"traning_no_vuoti.csv\",'w') as f:\n",
        "  writer = csv.writer(f)\n",
        "  for row in data:\n",
        "    if row[0]!= \"\":\n",
        "      writer.writerow(row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6y-cmLWXMeTf"
      },
      "source": [
        "file = open(\"/content/traning_clean.csv\")\n",
        "reader = csv.reader(file)\n",
        "lines = len(list(reader))\n",
        "print(lines)\n",
        "file1 = open(\"/content/traning_no_vuoti.csv\")\n",
        "reader1 = csv.reader(file1)\n",
        "lines1 = len(list(reader1))\n",
        "print(lines1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KE1Ef8XNHWaZ"
      },
      "source": [
        "#controllo frase più lunga dopo la pulizia\n",
        "\n",
        "len_embeding = 0\n",
        "for i in range(0,len(cleaned_text)):\n",
        "  if len_embeding < len(cleaned_text[i]):\n",
        "    len_embeding = len(cleaned_text[i])\n",
        "    \n",
        "print(len_embeding)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}